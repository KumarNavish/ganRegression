{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_cities(*(PosixPath('/gcsmount-research-data-staging/osmnx-cities/hexed-complete'),), **{'cities': ['Boston, USA', 'Austin, USA', 'Seattle, USA', 'Los Angeles, USA'], 'add_city_col': True})\n",
      "create_super_tags(*(), **{})\n",
      "Merge building_residential + building_house -> building_yes(*(), **{})\n",
      "drop_zero_buildings(*(), **{})\n",
      "drop_zero_building_area(*(), **{})\n",
      "add_new_special_tag(*('building.area.average',), **{})\n",
      "add_new_special_tag(*('parking.area.average',), **{})\n",
      "append_route_df(*(), **{'grouped': False, 'tukey_fences': False})\n",
      "open_route_df(*(), **{'grouped': False, 'tukey_fences': False})\n",
      "drop_zero_tags(*(), **{})\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "# hex2cec\n",
    "HOME = os.environ[\"HOME\"]\n",
    "\n",
    "sys.path.insert(0, f\"{os.environ['HOME']}/hex2vec\")\n",
    "sys.path.insert(0, f\"{os.environ['HOME']}/amazon-routing-challenge\")\n",
    "\n",
    "# add codebase\n",
    "sys.path.insert(0, f\"/gcsmount-notebook/codebase\")\n",
    "\n",
    "from src.data.make_dataset import h3_to_polygon\n",
    "import urban_tools.constants as uc\n",
    "import urban_tools.hex_pipeline as hp\n",
    "from urban_tools.hex_pipeline import RouteHexHandler, TestTrainManager\n",
    "from urban_tools.pipelines import route_hex_pipeline\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "import gcsfs\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "## Read in the DataFrame\n",
    "\n",
    "p = Path(\"/gcsmount-research-data-staging/osmnx-cities/hexed-routes/12.12.22-Revised-Embeddings/hh.pkl\")\n",
    "hh = RouteHexHandler.from_pickle(p)\n",
    "hh.print_history()\n",
    "hh = hh.drop_zero_tags()\n",
    "### Drop Tags that occur in <X% of a City\n",
    "# percentage = 0.01\n",
    "# import pandas as pd\n",
    "\n",
    "# drop_cols = pd.Index([])\n",
    "# for _hh in [hh]:\n",
    "#     assert _hh.df[\"city\"].unique().shape[0] == 1\n",
    "#     h3_df = _hh.df.groupby('h3')[_hh.all_tags].first()\n",
    "#     percent_occurance = (h3_df > 0).sum() / h3_df.shape[0]\n",
    "#     drop_cols = drop_cols.union(percent_occurance[percent_occurance < percentage].index)\n",
    "# hh.drop_cols(drop_cols)\n",
    "# len(hh.all_tags)\n",
    "### Append the embedding to the dataframe\n",
    "# embedding_df = pd.read_parquet(\n",
    "#     hh.my_folder / \"embeddings.prq\"\n",
    "# )\n",
    "embedding_df = pd.read_feather(\n",
    "    \"/gcsmount-research-data-staging/hex2vec-models/paper-final/embedding_dfs/subtags+cities+littlemodel.feather\"\n",
    ")\n",
    "embedding_df = embedding_df.set_index(\"h3\")\n",
    "# embedding_df.columns = [f\"e_{e}\" for e in embedding_df.columns]\n",
    "# embedding_df = embedding_df[embedding_df.columns.str.]\n",
    "# embedding_columns = embedding_df.columns.copy()\n",
    "# embedding_df = embedding_df.reset_index()\n",
    "\n",
    "hh.df = hh.df.merge(embedding_df, on=\"h3\", how=\"left\")\n",
    "hh.update_tags()\n",
    "hh.df = hh.df.dropna(subset=hh.embeddings)\n",
    "### Filter for only H3 with > X Data Points\n",
    "\n",
    "# tagged_df = tagged_df.loc[tagged_df[\"h3_9\"] > 20].copy()\n",
    "hh = hh.filter_hex_occurance(20)\n",
    "hh.df = hh.df[hh.df.city.str.contains('Boston')]\n",
    "# hh.df.drop(\"h3_9\", axis=1, inplace=True)\n",
    "# print(tagged_df.shape)\n",
    "hh.df = hh.df.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the TT Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn import metrics\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TestTrainManager(\n",
    "    hh.df,\n",
    "    x_col=hh.embeddings.union(hh.special_tags).union(hh.super_tags),\n",
    "    y_col=[\"planned_service_time_log\"],\n",
    "    scaler=RobustScaler,\n",
    "    grouped=False,\n",
    "    desired_quantiles=[0.1, 0.5, 0.90]\n",
    ")\n",
    "\n",
    "tt.split_test_train(train_size=0.8, random_seed=42)\n",
    "tt.scale_test_train()\n",
    "# split the tt.X_test into several chunks. Have to do this because of memory issues\n",
    "tt.build_test_df(agg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = tt.X_train.values, tt.Y_train.planned_service_time_log.values.ravel(), tt.X_test.values, tt.Y_test.loc.values\n",
    "X_train, X_cal, y_train, y_cal = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14135, 74), (3347, 74))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.X_train.shape, tt.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11308, 74), (2827, 74), (3347, 74))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_cal.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import statistics as stat\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngboost import NGBRegressor\n",
    "from ngboost.learners import default_tree_learner, default_linear_learner\n",
    "from ngboost.scores import CRPS, MLE \n",
    "from ngboost.distns import LogNormal, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb = NGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    Dist=Normal,\n",
    "    Base=default_tree_learner,\n",
    "    natural_gradient=True,\n",
    "    # minibatch_frac=1.0,\n",
    "    Score=MLE,\n",
    "    # minibatch_frac=0.5, \n",
    "    # col_sample=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGBRegressor on full train set. Dimensions of X_train:  (14135, 74)\n",
      "[iter 0] loss=1.0852 val_loss=0.0000 scale=1.0000 norm=0.8079\n",
      "[iter 100] loss=0.9945 val_loss=0.0000 scale=2.0000 norm=1.5021\n",
      "[iter 200] loss=0.9645 val_loss=0.0000 scale=1.0000 norm=0.7488\n",
      "[iter 300] loss=0.9527 val_loss=0.0000 scale=2.0000 norm=1.5005\n",
      "[iter 400] loss=0.9469 val_loss=0.0000 scale=1.0000 norm=0.7501\n",
      "[iter 500] loss=0.9430 val_loss=0.0000 scale=2.0000 norm=1.4993\n",
      "[iter 600] loss=0.9399 val_loss=0.0000 scale=1.0000 norm=0.7491\n",
      "[iter 700] loss=0.9374 val_loss=0.0000 scale=1.0000 norm=0.7486\n",
      "[iter 800] loss=0.9355 val_loss=0.0000 scale=2.0000 norm=1.4962\n",
      "[iter 900] loss=0.9336 val_loss=0.0000 scale=2.0000 norm=1.4953\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NGBRegressor(n_estimators=1000,\n",
       "             random_state=RandomState(MT19937) at 0x7F46AC1A1940)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NGBRegressor</label><div class=\"sk-toggleable__content\"><pre>NGBRegressor(n_estimators=1000,\n",
       "             random_state=RandomState(MT19937) at 0x7F46AC1A1940)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Base: DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(criterion=&#x27;friedman_mse&#x27;, max_depth=3)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(criterion=&#x27;friedman_mse&#x27;, max_depth=3)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "NGBRegressor(n_estimators=1000,\n",
       "             random_state=RandomState(MT19937) at 0x7F46AC1A1940)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.scale_test_train()\n",
    "print(\"NGBRegressor on full train set. Dimensions of X_train: \", tt.X_train.shape)\n",
    "\n",
    "ngb.fit(tt.X_train, tt.Y_train.values.ravel(), )  #X_val=tt.X_test, Y_val=tt.Y_test.loc.values.ravel(), early_stopping_rounds=200)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [11308, 14135]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/Regressors_copmarison_table.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdataproc-remote/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/Regressors_copmarison_table.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ngb_train_dist \u001b[39m=\u001b[39m ngb\u001b[39m.\u001b[39mpred_dist(tt\u001b[39m.\u001b[39mX_train)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdataproc-remote/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/Regressors_copmarison_table.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m nbg_train_MAPE \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39;49mmean_absolute_percentage_error(y_train, ngb_train_dist\u001b[39m.\u001b[39;49mloc)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdataproc-remote/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/Regressors_copmarison_table.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m nbg_train_R2 \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mr2_score(y_train,  ngb_train_dist\u001b[39m.\u001b[39mloc)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdataproc-remote/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/Regressors_copmarison_table.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m nbg_train_MSE \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mmean_squared_error(y_train,  ngb_train_dist\u001b[39m.\u001b[39mloc, squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_regression.py:365\u001b[0m, in \u001b[0;36mmean_absolute_percentage_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_absolute_percentage_error\u001b[39m(\n\u001b[1;32m    297\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m ):\n\u001b[1;32m    299\u001b[0m     \u001b[39m\"\"\"Mean absolute percentage error (MAPE) regression loss.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[39m    Note here that the output is not a percentage in the range [0, 100]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39m    112589990684262.48\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    366\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    367\u001b[0m     )\n\u001b[1;32m    368\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    369\u001b[0m     epsilon \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfinfo(np\u001b[39m.\u001b[39mfloat64)\u001b[39m.\u001b[39meps\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_regression.py:100\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     67\u001b[0m     \u001b[39m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m        correct keyword.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m    101\u001b[0m     y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    102\u001b[0m     y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [11308, 14135]"
     ]
    }
   ],
   "source": [
    "ngb_train_dist = ngb.pred_dist(tt.X_train)\n",
    "\n",
    "nbg_train_MAPE = metrics.mean_absolute_percentage_error(y_train, ngb_train_dist.loc)\n",
    "nbg_train_R2 = metrics.r2_score(y_train,  ngb_train_dist.loc)\n",
    "nbg_train_MSE = metrics.mean_squared_error(y_train,  ngb_train_dist.loc, squared=True)\n",
    "nbg_train_RMSE = metrics.mean_squared_error(y_train,  ngb_train_dist.loc, squared=False)\n",
    "\n",
    "print(f\"MAPE: {nbg_train_MAPE:.2f}, R2: {nbg_train_R2:.2f}, MSE: {nbg_train_MSE:.2f}, RMSE: {nbg_train_RMSE:.2f}\")\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_train,  ngb_train_dist.loc, s=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on the grouped train set\n",
    "h3_index = hh.df.loc[tt._test_slice, 'h3']\n",
    "x_test_grouped = tt.X_test.groupby(hh.df['h3'].iloc[h3_index]).first()\n",
    "y_test_grouped = tt.Y_test.groupby(hh.df['h3'].iloc[h3_index]).agg(('mean', 'std'))\n",
    "# drop level 0\n",
    "y_test_grouped.columns = y_test_grouped.columns.droplevel(0)\n",
    "\n",
    "y_pred_grouped = ngb.pred_dist(x_test_grouped)\n",
    "\n",
    "nbg_test_mean_MAPE = metrics.mean_absolute_percentage_error(y_test_grouped['mean'], y_pred_grouped.loc)\n",
    "nbg_test_mean_R2 = metrics.r2_score(y_test_grouped['mean'],  y_pred_grouped.loc)\n",
    "nbg_test_mean_MSE = metrics.mean_squared_error(y_test_grouped['mean'],  y_pred_grouped.loc, squared=True)\n",
    "nbg_test_mean_RMSE = metrics.mean_squared_error(y_test_grouped['mean'],  y_pred_grouped.loc, squared=False)\n",
    "\n",
    "print(f\"MAPE: {nbg_test_mean_MAPE:.2f}, R2: {nbg_test_mean_R2:.2f}, MSE: {nbg_test_mean_MSE:.2f}, RMSE: {nbg_test_mean_RMSE:.2f}\")\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_test_grouped['mean'],  y_pred_grouped.loc, s=10)\n",
    "\n",
    "#  testing on the scaled \n",
    "nbg_test_std_MAPE = metrics.mean_absolute_percentage_error(y_test_grouped['std'], y_pred_grouped.scale)\n",
    "nbg_test_std_R2 = metrics.r2_score(y_test_grouped['std'], y_pred_grouped.scale)\n",
    "nbg_test_std_MSE = metrics.mean_squared_error(y_test_grouped['std'], y_pred_grouped.scale, squared=True)\n",
    "nbg_test_std_RMSE = metrics.mean_squared_error(y_test_grouped['std'], y_pred_grouped.scale, squared=False)\n",
    "\n",
    "print(f\"MAPE: {nbg_test_std_MAPE:.2f}, R2: {nbg_test_std_R2:.2f}, MSE: {nbg_test_std_MSE:.2f}, RMSE: {nbg_test_std_RMSE:.2f}\")\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_test_grouped['std'], y_pred_grouped.scale, s=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22519293\n",
      "Validation score: 0.155506\n",
      "Iteration 2, loss = 0.21256592\n",
      "Validation score: 0.166689\n",
      "Iteration 3, loss = 0.20861406\n",
      "Validation score: 0.180832\n",
      "Iteration 4, loss = 0.20586463\n",
      "Validation score: 0.182109\n",
      "Iteration 5, loss = 0.20515468\n",
      "Validation score: 0.181525\n",
      "Iteration 6, loss = 0.20447696\n",
      "Validation score: 0.185493\n",
      "Iteration 7, loss = 0.20367596\n",
      "Validation score: 0.182074\n",
      "Iteration 8, loss = 0.20389400\n",
      "Validation score: 0.178804\n",
      "Iteration 9, loss = 0.20238575\n",
      "Validation score: 0.183528\n",
      "Iteration 10, loss = 0.20248813\n",
      "Validation score: 0.182719\n",
      "Iteration 11, loss = 0.20200614\n",
      "Validation score: 0.190579\n",
      "Iteration 12, loss = 0.20092184\n",
      "Validation score: 0.190191\n",
      "Iteration 13, loss = 0.20114971\n",
      "Validation score: 0.193537\n",
      "Iteration 14, loss = 0.20078023\n",
      "Validation score: 0.195553\n",
      "Iteration 15, loss = 0.20072532\n",
      "Validation score: 0.178089\n",
      "Iteration 16, loss = 0.20132224\n",
      "Validation score: 0.193952\n",
      "Iteration 17, loss = 0.20035361\n",
      "Validation score: 0.187834\n",
      "Iteration 18, loss = 0.20030165\n",
      "Validation score: 0.190291\n",
      "Iteration 19, loss = 0.19978057\n",
      "Validation score: 0.187595\n",
      "Iteration 20, loss = 0.20044521\n",
      "Validation score: 0.186956\n",
      "Iteration 21, loss = 0.20003013\n",
      "Validation score: 0.193745\n",
      "Iteration 22, loss = 0.19979895\n",
      "Validation score: 0.193013\n",
      "Iteration 23, loss = 0.19998375\n",
      "Validation score: 0.194503\n",
      "Iteration 24, loss = 0.19974981\n",
      "Validation score: 0.189948\n",
      "Iteration 25, loss = 0.19986808\n",
      "Validation score: 0.189369\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(early_stopping=True, hidden_layer_sizes=[16, 32, 128, 32, 16],\n",
       "             max_iter=1000, random_state=12323, validation_fraction=0.2,\n",
       "             verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(early_stopping=True, hidden_layer_sizes=[16, 32, 128, 32, 16],\n",
       "             max_iter=1000, random_state=12323, validation_fraction=0.2,\n",
       "             verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(early_stopping=True, hidden_layer_sizes=[16, 32, 128, 32, 16],\n",
       "             max_iter=1000, random_state=12323, validation_fraction=0.2,\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# 16, 32, 64, 128, 64, 32, 8\n",
    "# 16, 32, 128, 32, 16\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=[16, 32, 128, 32, 16],\n",
    "    max_iter=1000,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=1e-3,\n",
    "    random_state=12323,\n",
    "    verbose=True,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "mlp.fit(tt.X_train.values,tt.Y_train.values.ravel())\n",
    "# mlp.fit(tt.X_train.iloc[:batch_size,:].values,tt.Y_train.iloc[:batch_size,:].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-RMSE: 0.63, Train-R2: 0.22\n",
      "Test-RMSE: 0.65, Test-R2: 0.07\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train-RMSE: {mean_squared_error(tt.Y_train.values.ravel(), mlp.predict(tt.X_train.values), squared=False).round(2)}, Train-R2: {r2_score(tt.Y_train.values.ravel(), mlp.predict(tt.X_train.values)).round(2)}\")\n",
    "print(f\"Test-RMSE: {mean_squared_error(tt.Y_test.loc.values.ravel(), mlp.predict(tt.X_test.values), squared=False).round(2)}, Test-R2: {r2_score(tt.Y_test.loc.values.ravel(), mlp.predict(tt.X_test.values)).round(2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GP Regression (RBF Kernel + KISS GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e234d837eebba94c7397b4e38c0b82bd3e4741cb2c390182a3fb441eaf8f3cd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
