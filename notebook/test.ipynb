{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ext_navish_iitkgp_gmail_com/ganRegression/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# from functools import partial\n",
    "# import seaborn as sns\n",
    "# import plotly.graph_objects as go\n",
    "# import pandas as pd\n",
    "# import dask.dataframe as dd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import sys\n",
    "# import pickle\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "# # hex2vec\n",
    "# HOME = os.environ[\"HOME\"]\n",
    "\n",
    "# sys.path.insert(0, f\"{HOME}/hex2vec\")\n",
    "\n",
    "# # add codebase\n",
    "# sys.path.insert(0, f\"/gcsmount-notebook/codebase\")\n",
    "\n",
    "# from src.data.make_dataset import h3_to_polygon\n",
    "# import urban_tools.constants as uc\n",
    "# from urban_tools.hex_pipeline import RouteHexHandler, TestTrainManager\n",
    "# from urban_tools.pipelines import route_hex_pipeline\n",
    "\n",
    "\n",
    "# ## Read in the Delivery DataFrame\n",
    "# p = Path(\"/gcsmount-research-data-staging/osmnx-cities/hexed-routes/debug-multi-synthetic-tags/hh.pkl\")\n",
    "# hh = RouteHexHandler.from_pickle(p)\n",
    "# # embedding_df = pd.read_pickle(\"/gcsmount-research-data-staging/osmnx-cities/hexed-complete/Boston, MA/boston_embedding.pkl\")\n",
    "\n",
    "# ### Remove Super Tags\n",
    "# # hh.drop_super_tags()\n",
    "# ### Remove Sub Tags\n",
    "# # hh = hh.drop_sub_tags()\n",
    "# ### Filter for only H3 with > X Data Points\n",
    "\n",
    "# hh = hh.filter_hex_occurance(20)\n",
    "# ## Drop Unecessary Columns\n",
    "# hh.other_tags\n",
    "# ### Drop Chicago\n",
    "# # print(tagged_df.shape[0].compute(), tagged_df.shape[1])\n",
    "# # tagged_df = tagged_df.loc[~tagged_df[\"city\"].str.contains(\"Chicago\")]\n",
    "# # filter for only Boston\n",
    "# hh.df = hh.df.loc[hh.df.city.str.contains(\"Boston\")]\n",
    "# # print(tagged_df.shape[0].compute(), tagged_df.shape[1])\n",
    "\n",
    "# ## Data Preprocessing: Scaling and Splitting into train test\n",
    "\n",
    "\n",
    "# # create a grouped dataframe\n",
    "# tagged_df = hh.df.groupby(\"h3\").agg({\n",
    "#     \"planned_service_time_log\": \"mean\",\n",
    "#     **{\n",
    "#         tag: \"first\"\n",
    "#         for tag in hh.all_tags\n",
    "#     }\n",
    "# })\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# tt = TestTrainManager(\n",
    "#     tagged_df,\n",
    "#     x_col=hh.all_tags,\n",
    "#     y_col=[\"planned_service_time_log\"],\n",
    "#     scaler=RobustScaler,\n",
    "#     grouped=True,\n",
    "# )\n",
    "# tt.split_test_train(train_size=0.8, random_seed=6781)\n",
    "\n",
    "\n",
    "# tt.scale_test_train()\n",
    "# tt.build_test_df(agg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shape is (28337, 674)\n",
      "testing Data shape is (240, 674)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from numpy.random import binomial\n",
    "from numpy.random import normal\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# %reset -s -f\n",
    "import os\n",
    "from functools import partial\n",
    "import logging\n",
    "\n",
    "# hex2vec\n",
    "HOME = os.environ[\"HOME\"]\n",
    "\n",
    "sys.path.insert(0, f\"{HOME}/hex2vec\")\n",
    "\n",
    "# add codebase\n",
    "sys.path.insert(0, f\"/gcsmount-notebook/codebase\")\n",
    "\n",
    "from src.data.make_dataset import h3_to_polygon\n",
    "import urban_tools.constants as uc\n",
    "from urban_tools.hex_pipeline import RouteHexHandler, TestTrainManager\n",
    "from urban_tools.pipelines import route_hex_pipeline\n",
    "\n",
    "p = Path(\"/gcsmount-research-data-staging/osmnx-cities/hexed-routes/debug-multi-synthetic-tags/hh.pkl\")\n",
    "hh = RouteHexHandler.from_pickle(p)\n",
    "# embedding_df = pd.read_pickle(\"/gcsmount-research-data-staging/osmnx-cities/hexed-complete/Boston, MA/boston_embedding.pkl\")\n",
    "\n",
    "### Filter for only H3 with > X Data Points\n",
    "hh = hh.filter_hex_occurance(20)\n",
    "## Drop Unecessary Columns\n",
    "hh.other_tags\n",
    "\n",
    "## Data Preprocessing: Scaling and Splitting into train test\n",
    "\n",
    "#### Create the Test-Train Manager\n",
    "# filter for only Boston\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "hh.df = hh.df.loc[hh.df.city.str.contains(\"Boston\")]\n",
    "tt = TestTrainManager(\n",
    "        hh.df,\n",
    "        scaler=RobustScaler,\n",
    "        x_col=hh.all_tags,\n",
    "        y_col=hh.df.columns.intersection([\"planned_service_time_log\"]),\n",
    "    )\n",
    "\n",
    "tt.split_test_train(random_seed=12323)\n",
    "tt.scale_test_train()\n",
    "tt.build_test_df()\n",
    "train = pd.concat([tt.X_train, tt.Y_train], axis=1, copy=True)\n",
    "test = pd.concat([tt.X_test, tt.Y_test.loc, tt.Y_test.scale], axis=1, copy=True)\n",
    "\n",
    "print(f'Training Data shape is {tt.X_train.shape}')\n",
    "print(f'testing Data shape is {tt.X_test.shape}')\n",
    "\n",
    "del hh\n",
    "gc.collect()\n",
    "\n",
    "# convert a pandas dataframe to a pytorch dataset\n",
    "class PandasDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_df, y_df):\n",
    "        self._x_df = x_df.values.astype(np.float32)\n",
    "        self._y_df = y_df.values.astype(np.float32).ravel()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._x_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self._x_df[idx], self._y_df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall umapy\n",
    "# !pip install umap-learn\n",
    "# !pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mdn_model import MDN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = torch.Tensor(tt.X_train.values), torch.Tensor(tt.Y_train.values.ravel()), torch.Tensor(tt.X_test.values), torch.Tensor(tt.Y_test.loc.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([28337, 674]),\n",
       " torch.Size([28337]),\n",
       " torch.Size([240, 674]),\n",
       " torch.Size([240]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Variational GP Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "inducing_points = X_train[:500, :]\n",
    "model = GPModel(inducing_points=inducing_points)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef206e9a9454cc386b56caf1a0eb229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc4ae294bf2446397cb0d478925bd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-06.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotPSDError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/test.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdataproc-remote/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m minibatch_iter:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdataproc-remote/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdataproc-remote/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     output \u001b[39m=\u001b[39m model(x_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdataproc-remote/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmll(output, y_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdataproc-remote/home/ext_navish_iitkgp_gmail_com/ganRegression/notebook/test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     minibatch_iter\u001b[39m.\u001b[39mset_postfix(loss\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/models/approximate_gp.py:108\u001b[0m, in \u001b[0;36mApproximateGP.__call__\u001b[0;34m(self, inputs, prior, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m inputs\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    107\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariational_strategy(inputs, prior\u001b[39m=\u001b[39;49mprior, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/variational/variational_strategy.py:244\u001b[0m, in \u001b[0;36mVariationalStrategy.__call__\u001b[0;34m(self, x, prior, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[39m# Mark that we have updated the variational strategy\u001b[39;00m\n\u001b[1;32m    242\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdated_strategy\u001b[39m.\u001b[39mfill_(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 244\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(x, prior\u001b[39m=\u001b[39;49mprior, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/variational/_variational_strategy.py:302\u001b[0m, in \u001b[0;36m_VariationalStrategy.__call__\u001b[0;34m(self, x, prior, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39m# Get q(f)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(variational_dist_u, MultivariateNormal):\n\u001b[0;32m--> 302\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\n\u001b[1;32m    303\u001b[0m         x,\n\u001b[1;32m    304\u001b[0m         inducing_points,\n\u001b[1;32m    305\u001b[0m         inducing_values\u001b[39m=\u001b[39;49mvariational_dist_u\u001b[39m.\u001b[39;49mmean,\n\u001b[1;32m    306\u001b[0m         variational_inducing_covar\u001b[39m=\u001b[39;49mvariational_dist_u\u001b[39m.\u001b[39;49mlazy_covariance_matrix,\n\u001b[1;32m    307\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(variational_dist_u, Delta):\n\u001b[1;32m    310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\n\u001b[1;32m    311\u001b[0m         x, inducing_points, inducing_values\u001b[39m=\u001b[39mvariational_dist_u\u001b[39m.\u001b[39mmean, variational_inducing_covar\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    312\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/variational/variational_strategy.py:178\u001b[0m, in \u001b[0;36mVariationalStrategy.forward\u001b[0;34m(self, x, inducing_points, inducing_values, variational_inducing_covar, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m data_data_covar \u001b[39m=\u001b[39m full_covar[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, num_induc:, num_induc:]\n\u001b[1;32m    175\u001b[0m \u001b[39m# Compute interpolation terms\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39m# K_ZZ^{-1/2} K_ZX\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m# K_ZZ^{-1/2} \\mu_Z\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m L \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cholesky_factor(induc_induc_covar)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m L\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m induc_induc_covar\u001b[39m.\u001b[39mshape:\n\u001b[1;32m    180\u001b[0m     \u001b[39m# Aggressive caching can cause nasty shape incompatibilies when evaluating with different batch shapes\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39m# TODO: Use a hook fo this\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/utils/memoize.py:76\u001b[0m, in \u001b[0;36m_cached_ignore_args.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m cache_name \u001b[39m=\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m method\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_in_cache_ignore_args(\u001b[39mself\u001b[39m, cache_name):\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m _add_to_cache_ignore_args(\u001b[39mself\u001b[39m, cache_name, method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m     77\u001b[0m \u001b[39mreturn\u001b[39;00m _get_from_cache_ignore_args(\u001b[39mself\u001b[39m, cache_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/variational/variational_strategy.py:86\u001b[0m, in \u001b[0;36mVariationalStrategy._cholesky_factor\u001b[0;34m(self, induc_induc_covar)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m@cached\u001b[39m(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcholesky_factor\u001b[39m\u001b[39m\"\u001b[39m, ignore_args\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cholesky_factor\u001b[39m(\u001b[39mself\u001b[39m, induc_induc_covar):\n\u001b[0;32m---> 86\u001b[0m     L \u001b[39m=\u001b[39m psd_safe_cholesky(to_dense(induc_induc_covar)\u001b[39m.\u001b[39;49mtype(_linalg_dtype_cholesky\u001b[39m.\u001b[39;49mvalue()))\n\u001b[1;32m     87\u001b[0m     \u001b[39mreturn\u001b[39;00m TriangularLinearOperator(L)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:65\u001b[0m, in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpsd_safe_cholesky\u001b[39m(A, upper\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, jitter\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, max_tries\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m     \u001b[39m\"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m        :attr:`A` (Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m            Number of attempts (with successively increasing jitter) to make before raising an error.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     L \u001b[39m=\u001b[39m _psd_safe_cholesky(A, out\u001b[39m=\u001b[39;49mout, jitter\u001b[39m=\u001b[39;49mjitter, max_tries\u001b[39m=\u001b[39;49mmax_tries)\n\u001b[1;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m upper:\n\u001b[1;32m     67\u001b[0m         \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:47\u001b[0m, in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39many(info):\n\u001b[1;32m     46\u001b[0m         \u001b[39mreturn\u001b[39;00m L\n\u001b[0;32m---> 47\u001b[0m \u001b[39mraise\u001b[39;00m NotPSDError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMatrix not positive definite after repeatedly adding jitter up to \u001b[39m\u001b[39m{\u001b[39;00mjitter_new\u001b[39m:\u001b[39;00m\u001b[39m.1e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotPSDError\u001b[0m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-06."
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "num_epochs = 1 if smoke_test else 4\n",
    "\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=1e-4)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=y_train.size(0))\n",
    "\n",
    "\n",
    "epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    minibatch_iter = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e234d837eebba94c7397b4e38c0b82bd3e4741cb2c390182a3fb441eaf8f3cd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
